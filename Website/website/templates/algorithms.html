{% extends "content.html" %} {% block title %} DSA {% endblock %} {% block main_content %}
<h1>Algorithms</h1>

<p>
    Algorithms are a set of well-defined computational steps or instructions designed to transform input values into
    output values. Every piece of code can be considered an algorithm, but the field of algorithms focuses on those that
    are particularly fast or solve interesting problems.
</p>

<div class="subcontent">
    <h3>Why Algorithm Efficiency Matters</h3>
    <p>
        The study of algorithms is crucial because computers are not infinitely fast, and memory is not free.
        <strong
            >Efficient algorithms help in wisely using computational time and memory space, which are bounded
            resources</strong
        >. Different algorithms for the same problem can vary dramatically in their efficiency, often more significantly
        than differences in hardware and software.
        <strong
            >Total system performance depends as much on choosing efficient algorithms as it does on choosing fast
            hardware</strong
        >. Algorithms are at the core of most modern computer technologies.
    </p>
    <p>
        For large problem sizes, the differences in efficiency between algorithms become particularly prominent. For
        instance, a small improvement in efficiency can mean the difference between an operation taking 4 billion steps
        and just 32 steps. Understanding trade-offs between different algorithms (e.g., Merge Sort vs. Quicksort) and
        data structures (e.g., arrays vs. linked lists) is essential, as choosing the right one can make a significant
        difference in performance.
    </p>
</div>

<div class="subcontent">
    <h3>Measuring Algorithm Efficiency: Time and Space Complexity</h3>
    <p>
        Algorithm efficiency is typically measured by its
        <strong>time complexity</strong> (the number of operations an algorithm performs) and
        <strong>space complexity</strong> (the number of bytes in memory it occupies), both expressed as a function of
        the input size. <strong>Algorithm speed isn't measured in seconds</strong>, but rather in the
        <strong>growth rate of the number of operations</strong> as the input size increases.
    </p>
    <p>
        The input size itself depends on the problem: for sorting, it might be the array size <em>n</em>; for graph
        algorithms, it could be the number of vertices <em>V</em> and edges <em>E</em>.
    </p>
</div>
<div class="subcontent">
    <h3>Asymptotic Notations: Big O, Big Omega, and Big Theta</h3>
    <p>
        Asymptotic notations are mathematical notations used to
        <strong>compare the rate of convergence of functions</strong>. They describe the
        <strong>order of growth</strong> of an algorithm's running time, abstracting away constant factors and
        lower-order terms to focus on how the time increases for large inputs. They apply to functions defined over
        natural numbers.
    </p>
    <ol>
        <li>
            <p><strong>Big O Notation ($O$)</strong>:</p>
            <ul>
                <li>
                    <strong>Definition</strong>: Big O notation represents an <strong>asymptotic upper bound</strong> on
                    the running time of an algorithm. When we say $f(n) = O(g(n))$, it means that for sufficiently large
                    <code>n</code>, the function $f(n)$ is bounded above by some constant multiple of $g(n)$. Formally,
                    $f(n) = O(g(n))$ if there exist positive constants <code>c</code> and <code>n₀</code> such that $0 ≤
                    f(n) ≤ c \times g(n)$ for all $n ≥ n_0$.
                </li>
                <li>
                    <strong>Purpose</strong>: It provides a <strong>reassurance</strong> that an algorithm's running
                    time will never be slower than a certain rate. It captures the
                    <strong>worst-case scenario</strong> for an algorithm.
                </li>
                <li>
                    <strong>Common Use</strong>: Often used to describe the <strong>worst-case running time</strong>.
                    For example, Insertion Sort has a worst-case running time of $O(n^2)$. While $f(n) = O(g(n))$ means
                    an upper bound, in algorithm analysis, people sometimes use "Big O" informally to imply a tight
                    bound (which is more accurately Big Theta).
                </li>
                <li>
                    <strong>Examples of Common Big O Runtimes (Fastest to Slowest)</strong>:
                    <ul>
                        <li><strong>$O(\log{n})$</strong> (Logarithmic time): e.g., Binary Search.</li>
                        <li><strong>$O(n)$</strong> (Linear time): e.g., Simple Search.</li>
                        <li>
                            <strong>$O(n \log{n})$</strong>: e.g., fast sorting algorithms like Quicksort (average case)
                            or Merge Sort.
                        </li>
                        <li>
                            <strong>$O(n^2)$</strong> (Quadratic time): e.g., slow sorting algorithms like Selection
                            Sort.
                        </li>
                        <li><strong>$O(n!)$</strong> (Factorial time): e.g., the Traveling Salesperson Problem.</li>
                    </ul>
                </li>
            </ul>
        </li>
        <li>
            <p><strong>Big Omega Notation ($\Omega$)</strong>:</p>
            <ul>
                <li>
                    <strong>Definition</strong>: Big Omega notation is used for the
                    <strong>asymptotic lower bound</strong>. When we write $f(n) = \Omega(g(n))$, it means that for
                    sufficiently large <code>n</code>, $f(n)$ grows asymptotically no slower than $g(n)$. Formally,
                    $f(n) = \Omega(g(n))$ if there exist positive constants <code>c</code> and <code>n₀</code> such that
                    $0 ≤ c \times g(n) ≤ f(n)$ for all $n ≥ n_0$.
                </li>
                <li>
                    <strong>Purpose</strong>: It states that an algorithm's running time will be <em>at least</em> a
                    certain rate. It can be used when a tight bound cannot be proven.
                </li>
                <li>
                    <strong>Example</strong>: $f(n) = 3n^2 + 5n - 4$ is $\Omega(n^2)$, and also $\Omega(n)$ or
                    $\Omega(1)$. The best-case running time of Insertion Sort is $\Omega(n)$.
                </li>
            </ul>
        </li>
        <li>
            <p><strong>Big Theta Notation ($\Theta$)</strong>:</p>
            <ul>
                <li>
                    <strong>Definition</strong>: Big Theta notation represents a <strong>tight bound</strong>,
                    encompassing both an asymptotic upper and lower bound. When $f(n) = \Theta(g(n))$, it means that
                    $f(n)$ and $g(n)$ grow at the same rate, or "behave similarly" for large enough values of
                    <code>n</code>. Formally, $f(n) = \Theta(g(n))$ if there exist positive constants <code>c₁</code>,
                    <code>c₂</code>, and <code>n₀</code> such that $0 ≤ c_1 \times g(n) ≤ f(n) ≤ c_2 \times g(n)$ for
                    all $n ≥ n₀$.
                </li>
                <li>
                    <strong>Relationship</strong>:
                    <strong
                        >An algorithm $f(n)$ is $\Theta(g(n))$ if and only if it is both $O(g(n))$ and
                        $\Omega(g(n))$</strong
                    >. This makes it more precise than just Big O, but also more difficult to compute.
                </li>
                <li>
                    <strong>Example</strong>: An algorithm taking $42n^2 + 25n + 4$ operations is $\Theta(n^2)$, but not
                    $\Theta(n^3)$. Merge Sort's worst-case running time is $\Theta(n \log{n})$.
                </li>
            </ul>
        </li>
    </ol>
</div>
<div class="subcontent">
    <h3>T Notation</h3>
    <p>
        The notation $T(n)$ is commonly used in algorithm analysis to denote the
        <strong>running time</strong> of an algorithm as a function of the input size <em>n</em>. It is not a separate
        asymptotic notation like Big O, Big Omega, or Big Theta, but rather the function whose asymptotic growth is
        described by these notations. For example, $T(n) = 2T(n/2) + O(n)$ is a recurrence relation where
        <code>T(n)</code> represents the running time, and its solution is $T(n) = \Theta(n \log{n})$.
    </p>
</div>
<div class="subcontent">
    <h3>Calculating Efficiency: Loops and Recursion</h3>
    <h4>Efficiency in Loops</h4>
    <ol>
        <li>
            <strong>Simple Loop</strong>: To find the maximal element in an array of <code>len</code> (or
            <code>n</code>) elements, a simple loop iterates <code>n</code> times. If the operations inside the loop
            take a constant amount of time (e.g., 3 operations per loop), the total operations can be expressed as $3n +
            2$ (including initial assignments). As <code>n</code> grows, this is considered $O(n)$, or linear time,
            because the operations grow proportionally to the input size.
        </li>
        <li>
            <strong>Nested Loop</strong>: When a loop is nested inside another loop, the number of operations increases
            significantly. For example, a function checking for duplicates in an array might use two nested loops, each
            running <code>n</code> times. Operations inside the inner loop run <code>n²</code> times, leading to a
            complexity like $an² + bn + c$, which is $O(n^2)$, or quadratic time. Even if optimizations reduce the total
            operations (e.g., $\frac{n(n-1)}{2}$ for the inner loop), if the highest term remains <code>n²</code>, it
            stays in the same $O(n^2)$ complexity class.
        </li>
        <li>
            <strong>Logarithmic Loop ($O(log n)$)</strong>: Some algorithms reduce the problem size by a constant factor
            (e.g., halving it) in each step. If a problem of size <code>n</code> becomes <code>n/2</code>, then
            <code>n/4</code>, and so on, until it reaches a size of <code>1</code>, the number of steps
            <code>k</code> required is such that $\frac{n}{2} = 1$, meaning $n = 2$. Taking the logarithm (base 2) of
            both sides gives $k = \log_2{n}$. Therefore, the time complexity is $O(\log{n})$, or logarithmic time. This
            is considerably faster than linear time, especially for large <code>n</code>. An example is a loop where the
            control variable <code>i</code> is repeatedly multiplied by 2 (e.g., <code>i = i * 2</code>).
        </li>
    </ol>
    <h4>Efficiency in Recursion</h4>
    <p>
        Recursive algorithms often divide a problem into smaller instances of the same problem, leading to their running
        times being described by
        <strong>recurrence equations (or recurrences)</strong>.
    </p>
    <ol>
        <li>
            <strong>Divide-and-Conquer (D&C)</strong>: This is a common algorithmic design paradigm where a problem is
            broken down into smaller, similar subproblems, solved recursively, and then their solutions are combined.
            <ul>
                <li>
                    <strong>Base Case</strong>: A recursive function must have a base case, which is the simplest input
                    that can be solved directly, preventing infinite loops. For list-based D&C, this is often an empty
                    array or an array with one element.
                </li>
                <li>
                    <strong>Recursive Case</strong>: The function calls itself with a smaller version of the problem.
                </li>
                <li>
                    <strong>Example: Merge Sort</strong>: This algorithm sorts <code>n</code> numbers by recursively
                    sorting two subarrays of <code>n/2</code> elements each and then merging the sorted subarrays. The
                    running time is described by the recurrence $T(n) = 2T(n/2) + O(n)$. The $O(n)$ term represents the
                    time taken for the "divide" (constant time) and "combine" (linear time for merging) steps. This
                    recurrence has a solution of $O(n \log{n})$.
                </li>
            </ul>
        </li>
        <li>
            <strong>Recursion-Tree Method</strong>: This method provides a straightforward way to devise a good guess
            for a recurrence solution. Each node in the tree represents the cost of a single subproblem, and summing
            costs at each level gives per-level costs, which are then summed to find the total cost.
        </li>
        <li>
            <strong>Master Theorem</strong>: This theorem offers a <strong>"cookbook"</strong> method for solving
            recurrences of the form $T(n) = aT(n/b) + f(n)$, where <code>a ≥ 1</code> and <code>b > 1</code> are
            constants, and $f(n)$ is an asymptotically positive function. The theorem has three cases, comparing $f(n)$
            with $n^{(\log_b{a})}$:
            <ul>
                <li>
                    <strong>Case 1</strong>: If $f(n)$ is polynomially smaller than $n^{(\log_b{a})}$, then $T(n) =
                    \Theta(n^{(\log_b{a})})$.
                    <ul>
                        <li>
                            <em>Example</em>: For <code>T(n) = 8T(n/2) + O(n)</code> (matrix multiplication), $\log_2{8}
                            = 3$, so $n$ dominates $n$. Thus $T(n) = \Theta(n)$.
                        </li>
                        <li>
                            <em>Example</em>: For <code>T(n) = 7T(n/2) + O(n)</code> (Strassen's algorithm), $\log_2{7}
                            \approx 2.81 \therefore n^{(\log_2{7})}$ dominates $n$. Thus $T(n) = \Theta(n^{\log_2{7}})$.
                        </li>
                    </ul>
                </li>
                <li>
                    <strong>Case 2</strong>: If $f(n)$ is asymptotically the same size as $n^{(\log_b{a})}$, then $T(n)
                    = \Theta(n^{(\log_b{a})} \log{n})$.
                    <ul>
                        <li>
                            <em>Example</em>: For <code>T(n) = 2T(n/2) + O(n)</code> (Merge Sort or maximum-subarray
                            problem), $log_2{2} = 1$, so $n$ is comparable to $n$. Thus $T(n) = \Theta(n \log{n})$.
                        </li>
                        <li>
                            <em>Example</em>: For <code>T(n) = T(n/2) + O(1)</code> (Binary Search), $\log_2{1} = 0$, so
                            $n^{\log_2{1}} = 1$ is comparable to $O(1)$. Thus $T(n) = \Theta(\log{n})$.
                        </li>
                    </ul>
                </li>
                <li>
                    <strong>Case 3</strong>: If $f(n)$ is polynomially larger than $n^{(\log_b{a})}$ and satisfies a
                    regularity condition, then $T(n) = \Theta(f(n))$.
                </li>
            </ul>
        </li>
    </ol>
</div>
<div class="subcontent">
    <h3>Types of Algorithms</h3>
    <ul>
        <li>
            <p>
                <strong><a href="{{ url_for('views.searching_algorithms') }}">Searching Algorithms</a> : </strong>
                These algorithms are used to find a specific item or a set of items with particular properties within a
                larger collection of data.
            </p>
        </li>
        <li>
            <p>
                <strong><a href="Sorting%20Algorithms">Sorting Algorithms</a> : </strong>
                These algorithms arrange elements of a list or array in a specific order (e.g., numerical,
                alphabetical).
            </p>
        </li>
        <li>
            <p>
                <strong>Graph Algorithms : </strong>These algorithms are designed to solve problems modelled as a
                network of interconnected entities (nodes/vertices and edges). They are fundamental for tasks involving
                relationships, connections, and paths.
            </p>
        </li>
        <li>
            <p>
                <strong>Dynamic Programming : </strong>
                This technique solves
                <strong>optimization problems</strong> by breaking them into
                <strong>overlapping subproblems</strong> and solving each subproblem only once, storing its solution to
                avoid re-computation. It builds an optimal solution to the problem from optimal solutions to
                subproblems. Every dynamic-programming algorithm typically starts with a <strong>grid</strong>.
            </p>
        </li>
        <li>
            <p>
                <strong>Greedy Algorithms : </strong>
                These algorithms solve optimization problems by making a sequence of
                <strong>locally optimal choices</strong> at each step, hoping to lead to a globally optimal solution.
                They don't always yield the global optimum, but for many problems, they do. A key characteristic is that
                a greedy choice typically leaves only <strong>one subproblem</strong> to solve.
            </p>
        </li>
        <li>
            <p>
                <strong>Divide-and-Conquer Algorithms (General Paradigm)</strong> This paradigm solves problems by
                recursively breaking them into smaller subproblems of the same type, solving these subproblems
                independently, and then combining their solutions to solve the original problem. This strategy often
                yields efficient algorithms.
            </p>
        </li>
        <li>
            <p>
                <strong>String Matching Algorithms</strong> These algorithms find occurrences of a smaller string
                (pattern) within a larger text string.
            </p>
        </li>
        <li>
            <p>
                <strong>Linear Programming</strong>
                These algorithms solve optimization problems where one aims to maximize or minimize a
                <strong>linear objective function</strong> subject to a set of
                <strong>linear inequality or equality constraints</strong>. They are used for resource allocation and
                optimization.
            </p>
        </li>
        <li>
            <p>
                <strong>Hashing</strong> Hash tables store key-value pairs, providing very fast (average O(1) time)
                lookups, insertions, and deletions. They are useful for modelling relationships, filtering out
                duplicates, and caching data.
            </p>
        </li>
    </ul>
    <p>
        *This comprehensive list highlights the diverse and interconnected nature of algorithms, ranging from
        foundational concepts like searching and sorting to advanced topics in optimisation, parallel processing, and
        data structure design.
    </p>
</div>

{% endblock %}
